{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5c947be-cb55-4266-bdc1-909247b6f077",
   "metadata": {},
   "source": [
    "This script is used to format the data presented in supp. mat. tables 1 & 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643146f9-722e-400f-940f-3c903c1b7815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "\n",
    "#custom\n",
    "sys.path.append('./../../lib')\n",
    "import paths as paths\n",
    "import utils as utils\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcdeabd-6f47-4f67-9748-fac4100afcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_base = Path(paths.dir_main)\n",
    "\n",
    "dir_out_figures = dir_base / \"outputs\"\n",
    "\n",
    "# all the inputs are actually in the 'outputs' folder, as they were generated by other analysis scripts\n",
    "fp_vpd1 = dir_out_figures / \"s4_timeseries_gfed_vpd_flip.csv\"\n",
    "fp_fw1 = dir_out_figures / \"s5_timeseries_gfed_fwsl_flip.csv\"\n",
    "fp_vpd2 = dir_out_figures / \"s4_timeseries_TTB_vpd_flip.csv\"\n",
    "fp_fw2 = dir_out_figures / \"s5_timeseries_TTB_fwsl_flip.csv\"\n",
    "fp_sl = dir_out_figures / \"s6_timeseries_season_length.csv\"\n",
    "fp_pi = dir_out_figures / \"s6_timeseries_peak_value.csv\"\n",
    "\n",
    "# both vpd and fw dataframes also contain the avhrr data\n",
    "vpd1=pd.read_csv(str(fp_vpd1))\n",
    "vpd2=pd.read_csv(str(fp_vpd2))\n",
    "fw1=pd.read_csv(str(fp_fw1))\n",
    "fw2=pd.read_csv(str(fp_fw2))\n",
    "sl = pd.read_csv(str(fp_sl))\n",
    "pi = pd.read_csv(str(fp_pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a808727-0dbc-4751-a70b-768d2a9f6070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# both vpd and fw also contain avhrr, don't need it in both\n",
    "vpd1 = vpd1.loc[vpd1.metric=='fireweather'].copy()\n",
    "vpd2 = vpd2.loc[vpd2.metric=='fireweather'].copy()\n",
    "vpd1['metric'] = 'vpd' #labelled 'fireweather'\n",
    "vpd2['metric'] = 'vpd'\n",
    "\n",
    "#split out fw and avhrr\n",
    "av1 = fw1.loc[fw1.metric=='avhrr'].copy()\n",
    "av2 = fw2.loc[fw2.metric=='avhrr'].copy()\n",
    "fw1 = fw1.loc[fw1.metric=='fireweather'].copy()\n",
    "fw2 = fw2.loc[fw2.metric=='fireweather'].copy()\n",
    "\n",
    "# seperate out season length and peak intensity\n",
    "sl['metric'] = 'season_length'\n",
    "pi['metric'] = 'peak_value'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbf4346-ad77-439f-a1b6-914069e4eaa3",
   "metadata": {},
   "source": [
    "### Make Table 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3a635f-9404-47fe-a2a7-c08a6a769d62",
   "metadata": {},
   "source": [
    "first combine the GFED data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470b39b6-c935-4a28-8fc5-1280c3c2a1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate([av1, sl, pi, fw1, vpd1]):\n",
    "\n",
    "    # first apply rounding\n",
    "    df['tau'] = np.round(df['tau'],2)\n",
    "    df['p'] = np.round(df['p'],3)\n",
    "    df['slope'] = np.round(df['slope'],3)\n",
    "    df['lower'] = np.round(df['lower'],3)\n",
    "    df['upper'] = np.round(df['upper'],3)    \n",
    "    \n",
    "    # combine dates\n",
    "    df['period'] = df.year_start.astype(int).astype(str) + '-' + df.year_end.astype(int).astype(str)\n",
    "\n",
    "    ## format tau and p\n",
    "    df['tau_str'] = df['tau'].map(lambda x :f'{np.round(x,2)}') # even though already rounded, float precision issues seems to require this to be rounded again when convert to string or otherwise not precise\n",
    "    df['p_str'] = df['p'].map(lambda x :f'{np.round(x,3):0<5}')  \n",
    "    df['stars'] = ''\n",
    "\n",
    "    df.loc[df.p >= 0.1, 'tau_str'] = 'NS'\n",
    "    df.loc[df.p < 0.001, 'p_str'] = '< 0.001'\n",
    "    df.loc[df.p < 0.1, 'stars'] = '*'\n",
    "    df.loc[df.p < 0.05, 'stars'] = '**'\n",
    "    df.loc[df.p < 0.01, 'stars'] = '***'\n",
    "\n",
    "    df['tau_p'] = df.tau_str + \" (\" + df.p_str + df.stars + \")\"\n",
    "\n",
    "    # Trend (slope*100) and CI\n",
    "    df['slope_str'] = df['slope'].map(lambda x :f'{np.round(x*100, 1)}') \n",
    "    df['lower_str'] = df['lower'].map(lambda x :f'{np.round(x*100, 1)}') \n",
    "    df['upper_str'] = df['upper'].map(lambda x :f'{np.round(x*100, 1)}') \n",
    "    df['trend_ci'] = df['slope_str'] + \" [\" + df['lower_str'] + \", \" + df['upper_str'] + \"]\"\n",
    "    df.loc[df.p >= 0.1, 'trend_ci'] = ''\n",
    "    \n",
    "    # combine dfs\n",
    "    prefix = df.metric.values[0]\n",
    "    df = df[['region', 'period', 'tau_p', 'trend_ci']]\n",
    "    df = df.rename(columns={col: col + '_' + prefix    \n",
    "                            for col in df.columns if col not in ['region', 'period']})  \n",
    "    if i == 0:\n",
    "        df_out = df.copy()\n",
    "    else:\n",
    "        df_out = df_out.merge(df,on=['region','period'], how='outer')\n",
    "    #print(df)\n",
    "\n",
    "df_out1 = df_out.copy()\n",
    "df_out1 = df_out1[df_out1.region!='global']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdd4209-9112-4818-bca0-caba7ea0d721",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "now do for biomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948cfb59-29b9-4d7b-b2dc-b0702c78fdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate([av2, fw2, vpd2]):\n",
    "    # first apply rounding\n",
    "    df['tau'] = np.round(df['tau'],2)\n",
    "    df['p'] = np.round(df['p'],4)\n",
    "    df['slope'] = np.round(df['slope'],3)\n",
    "    df['lower'] = np.round(df['lower'],3)\n",
    "    df['upper'] = np.round(df['upper'],3)  \n",
    "    \n",
    "    # combine dates\n",
    "    df['period'] = df.year_start.astype(int).astype(str) + '-' + df.year_end.astype(int).astype(str)\n",
    "\n",
    "    ## format tau and p\n",
    "    df['tau_str'] = df['tau'].map(lambda x :f'{np.round(x,2)}') # even though already rounded, float precision issues seems to require this to be rounded again when convert to string or otherwise not precise\n",
    "    df['p_str'] = df['p'].map(lambda x :f'{np.round(x,3):0<5}') \n",
    "    df['stars'] = ''\n",
    "\n",
    "    df.loc[df.p >= 0.1, 'tau_str'] = 'NS'\n",
    "    df.loc[df.p < 0.001, 'p_str'] = '< 0.001'\n",
    "    df.loc[df.p < 0.1, 'stars'] = '*'\n",
    "    df.loc[df.p < 0.05, 'stars'] = '**'\n",
    "    df.loc[df.p < 0.01, 'stars'] = '***'\n",
    "\n",
    "    df['tau_p'] = df.tau_str + \" (\" + df.p_str + df.stars + \")\"\n",
    "\n",
    "    # Trend (slope*100) and CI\n",
    "    df['slope_str'] = df['slope'].map(lambda x :f'{np.round(x*100, 1)}') \n",
    "    df['lower_str'] = df['lower'].map(lambda x :f'{np.round(x*100, 1)}') \n",
    "    df['upper_str'] = df['upper'].map(lambda x :f'{np.round(x*100, 1)}') \n",
    "    df['trend_ci'] = df['slope_str'] + \" [\" + df['lower_str'] + \", \" + df['upper_str'] + \"]\"\n",
    "    df.loc[df.p >= 0.1, 'trend_ci'] = ''\n",
    "    \n",
    "    # combine dfs\n",
    "    prefix = df.metric.values[0]\n",
    "    df = df[['region', 'period', 'tau_p', 'trend_ci']]\n",
    "    df = df.rename(columns={col: col + '_' + prefix    \n",
    "                            for col in df.columns if col not in ['region', 'period']})  \n",
    "    if i == 0:\n",
    "        df_out = df.copy()\n",
    "    else:\n",
    "        df_out = df_out.merge(df,on=['region','period'], how='outer')\n",
    "    #print(df)\n",
    "\n",
    "df_out2 = df_out.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3fd2c4-22a9-4611-abc0-084516f858b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feae566-f035-41c2-b582-3a071cd0b456",
   "metadata": {},
   "outputs": [],
   "source": [
    "table1 = pd.concat([df_out2.loc[df_out2['region']=='Global'], \n",
    "                    df_out2.loc[df_out2['region']!='Global'], \n",
    "                    df_out1]).reset_index(drop=True)\n",
    "\n",
    "table1.to_csv(str(dir_out_figures / \"table1.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23de6bf9-81af-4cdf-94ca-add687327b83",
   "metadata": {},
   "source": [
    "### Make Table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab29bda-db38-4116-80f4-9fa1e6a24dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_anf = dir_out_figures / '04_stats_for_table2_anf.csv'\n",
    "fp_pi = dir_out_figures / '04_stats_for_table2_pfam.csv'\n",
    "fp_sl = dir_out_figures / '04_stats_for_table2_season_length.csv'\n",
    "fp_vpd = dir_out_figures / '04_stats_for_table2_vpd.csv'\n",
    "fp_fwsl = dir_out_figures / '04_stats_for_table2_fwsl.csv'\n",
    "\n",
    "anf = pd.read_csv(str(fp_anf))\n",
    "pi = pd.read_csv(str(fp_pi))\n",
    "sl = pd.read_csv(str(fp_sl))\n",
    "vpd = pd.read_csv(str(fp_vpd))\n",
    "fdi = pd.read_csv(str(fp_fwsl))\n",
    "\n",
    "anf['var'] = 'anf'\n",
    "sl['var'] = 'season_length'\n",
    "pi['var'] = 'pfam'\n",
    "vpd['var'] = 'vpd_mean'\n",
    "fdi['var'] = 'fwsl_mean'\n",
    "\n",
    "# if global, remove\n",
    "anf = anf[anf['gfed_name']!='global']\n",
    "pi = pi[pi['gfed_name']!='global']\n",
    "sl = sl[sl['gfed_name']!='global']\n",
    "vpd = vpd[vpd['gfed_name']!='Global'] #note capital 'G'\n",
    "fdi = fdi[fdi['gfed_name']!='Global'] #note capital 'G'\n",
    "\n",
    "anf.loc[anf.gfed_name=='SEAS', 'gfed_name'] = 'SOAS' # in case of residual region name typo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b321677a-ccc3-4615-aa21-8d9e18910068",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate([anf, sl, pi, fdi, vpd]):\n",
    "    if df['var'].iloc[0] == 'vpd_mean': #need to convert kPa -> Pa or everything rounded to 0 or 1\n",
    "        df['50%'] = df['50%'] * 1000\n",
    "        df['25%'] = df['25%'] * 1000\n",
    "        df['75%'] = df['75%'] * 1000\n",
    "    # first apply rounding  \n",
    "    df['50%'] = np.round(df['50%'],0)\n",
    "    df['25%'] = np.round(df['25%'],0)\n",
    "    df['75%'] = np.round(df['75%'],0)\n",
    "    df['mwu_2side'] = np.round(df['mwu_2side'],0)\n",
    "    df['mwu_2side_p'] = np.round(df['mwu_2side_p'],3)\n",
    "    \n",
    "    ## format median and IQR\n",
    "    df['med_str'] = df['50%'].map(lambda x :f'{int(x)}') \n",
    "    df['lower_str'] = df['25%'].map(lambda x :f'{int(x)}') \n",
    "    df['upper_str'] = df['75%'].map(lambda x :f'{int(x)}') \n",
    "    df['miqr'] = df['med_str'] + \" [\" + df['lower_str'] + \", \" + df['upper_str'] + \"]\"\n",
    "\n",
    "    # get MWU statistic and p value\n",
    "    df['mwu_str'] = df['mwu_2side'].map(lambda x :f'{int(x)}') \n",
    "    df['p_str'] = df['mwu_2side_p'].map(lambda x :f'{x:0<5}') \n",
    "    df['stars'] = ''\n",
    "\n",
    "    df.loc[df.mwu_2side_p >= 0.1, 'mwu_str'] = 'NS'\n",
    "    df['stars'] = ''\n",
    "    df.loc[df.mwu_2side_p < 0.001, 'p_str'] = '< 0.001'\n",
    "    df.loc[df.mwu_2side_p < 0.1, 'stars'] = '*'\n",
    "    df.loc[df.mwu_2side_p < 0.05, 'stars'] = '**'\n",
    "    df.loc[df.mwu_2side_p < 0.01, 'stars'] = '***'\n",
    "    df['mwu_p'] = df.mwu_str + \" (\" + df.p_str + df.stars + \")\"\n",
    "\n",
    "    # add iqr and calc % changes\n",
    "    df['iqr'] = df['75%']-df['25%']\n",
    "    df1 = df.loc[df.era==1]\n",
    "    df2 = df.loc[df.era==2]\n",
    "    df3 = df1[['gfed','gfed_name','50%','iqr']].merge(df2[['gfed','gfed_name','50%','iqr']], on=['gfed','gfed_name'])\n",
    "    df3['med_pc'] = np.round((df3['50%_y'] - df3['50%_x'])/df3['50%_x']*100,0)\n",
    "    df3['iqr_pc'] = np.round((df3['iqr_y'] - df3['iqr_x'])/df3['iqr_x']*100,0)\n",
    "    df = df.merge(df3[['gfed', 'gfed_name', 'med_pc', 'iqr_pc']], on=['gfed','gfed_name'])        \n",
    "    \n",
    "    # no need for duplication of some stats\n",
    "    df.loc[df.era==2,'mwu_p'] = ''\n",
    "    df.loc[df.era==2,'med_pc'] = ''\n",
    "    df.loc[df.era==2,'iqr_pc'] = ''\n",
    "    \n",
    "    # combine dfs\n",
    "    prefix = df['var'].values[0]\n",
    "    df = df[['gfed', 'gfed_name', 'era', 'miqr', 'mwu_p', '50%', 'iqr', 'med_pc', 'iqr_pc']]\n",
    "    df = df.rename(columns={col: col + '_' + prefix    \n",
    "                            for col in df.columns if col not in ['gfed', 'gfed_name', 'era']})\n",
    "    if i == 0:\n",
    "        df_out = df.copy()\n",
    "    else:\n",
    "        df_out = df_out.merge(df,on=['gfed', 'gfed_name', 'era'], how='outer')\n",
    "    #print(df)\n",
    "\n",
    "# relabel\n",
    "df_out=df_out.sort_values(['gfed', 'era'], ascending=True)\n",
    "df_out.loc[df_out.era==1,'era'] = '1986-2000'\n",
    "df_out.loc[df_out.era==2,'era'] = '2001-2016'\n",
    "df_out.to_csv(str(dir_out_figures / \"table2.csv\"), index=False)\n",
    "df_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
